{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> f4284c7e72f458e236b74821ddb83ff0631b536a
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nlp516\n",
    "import nlp516.model\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from types import SimpleNamespace\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> f4284c7e72f458e236b74821ddb83ff0631b536a
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = nlp516.data.PublicTrialRaw()\n",
    "language = 'english'\n",
    "if language == 'spanish':\n",
    "    raw = nlp516.data.PublicSpanishDataset()\n",
    "elif language=='english':\n",
    "    raw = nlp516.data.PublicEnglishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @ArianasBotch Ok if you fucking said leave block me. But dm me first I'm gonna kick your ass. Shut the fuck up you https://t.co/6BXkfxMEf9\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9c73ebc42baf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Original: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokens: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'en' is not defined"
     ]
    }
   ],
   "source": [
    "print('Original: {}'.format(dataset.en.iloc[1].text))\n",
    "print('Tokens: {}'.format(en.iloc[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
>>>>>>> f4284c7e72f458e236b74821ddb83ff0631b536a
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_map = nlp516.data.Tokenizer('english')\n",
    "remove_stopwords_map = nlp516.data.RemoveStopWords(language)\n",
    "stemmer_map = nlp516.data.Stemmer(language)\n",
    "\n",
    "def preprocess(dataset):\n",
    "    def run(data):\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.remove_urls_map)\n",
    "        #en = nlp516.data.map_column(dataset.en, 'text', nlp516.data.casual_tokenize_map)\n",
    "        data = nlp516.data.map_column(data, 'text', tokenizer_map)\n",
    "        #data = nlp516.data.map_column(data, 'text', nlp516.data.user_camelcase_map)\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.remove_user_map)\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.hashtag_camelcase_map)\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.to_lowercase)\n",
    "        data = nlp516.data.map_column(data, 'text', remove_stopwords_map)\n",
    "        data = nlp516.data.map_column(data, 'text', stemmer_map)\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.remove_words_with_numbers)\n",
    "        data = nlp516.data.map_column(data, 'text', nlp516.data.remove_punctuation)\n",
    "        return data\n",
    "    return SimpleNamespace(train = run(dataset.train),\n",
    "                           valid = run(dataset.valid))\n",
    "dataset = preprocess(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Preprocessing"
=======
    "print('Original: {}'.format(raw.train.iloc[25].text))\n",
    "print('Tokens: {}'.format(dataset.train.iloc[25].text))"
>>>>>>> f4284c7e72f458e236b74821ddb83ff0631b536a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bbfe08232ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Original: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokens: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print('Original: {}'.format(dataset.en.iloc[25].text))\n",
    "print('Tokens: {}'.format(en.iloc[25].text))"
=======
    "print('Original: {}'.format(raw.train.iloc[1].text))\n",
    "print('Tokens: {}'.format(dataset.train.iloc[1].text))"
>>>>>>> f4284c7e72f458e236b74821ddb83ff0631b536a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original: {}'.format(raw.train.iloc[26].text))\n",
    "print('Tokens: {}'.format(dataset.train.iloc[26].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "import sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def subtask_dataset(dataset, task):\n",
    "    train = SimpleNamespace(x=dataset.train.text,\n",
    "                                y=getattr(dataset.train, task))\n",
    "    valid = SimpleNamespace(x=dataset.valid.text,\n",
    "                            y=getattr(dataset.valid, task))\n",
    "    return SimpleNamespace(train=train, valid=valid)\n",
    "\n",
    "def eval_metrics(model, dataset):\n",
    "    model.fit(dataset.train.x, dataset.train.y)\n",
    "    return {'accuracy': model.score(dataset.valid.x, dataset.valid.y),\n",
    "            'precision': model.precision_score(dataset.valid.x, dataset.valid.y),\n",
    "            'recall': model.recall_score(dataset.valid.x, dataset.valid.y),\n",
    "            'f1': model.f1_score(dataset.valid.x, dataset.valid.y)}\n",
    "\n",
    "def instantiate_models(classifiers, vectorizers):\n",
    "    models = {('MajorityBaseline', '-'): nlp516.model.MajorityBaseline()}\n",
    "    models.update(\n",
    "        {(c, v): nlp516.model.MlModel(classifier=classifiers[c](), \n",
    "                                      vectorizer=vectorizers[v]())\n",
    "         for c, v in itertools.product(classifiers.keys(), vectorizers.keys())\n",
    "        }\n",
    "    )\n",
    "    return models\n",
    "\n",
    "def eval_models(classifiers, vectorizers, task, dataset):\n",
    "    models = instantiate_models(classifiers, vectorizers)\n",
    "    results = {key: eval_metrics(model, dataset=subtask_dataset(dataset, task))\n",
    "               for key, model in models.items()}\n",
    "    return pd.DataFrame(results).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'AG'\n",
    "classifiers={'linear': lambda: sklearn.linear_model.LogisticRegression(),\n",
    "             'svm': lambda: sklearn.svm.SVC(gamma='scale'),\n",
    "             'tree': lambda: sklearn.tree.DecisionTreeClassifier(),\n",
    "             'bayes': lambda: sklearn.naive_bayes.GaussianNB()}\n",
    "#vectorizers = {'frequency': lambda: nlp516.vectorizer.Unigram2(1000)} #,\n",
    "vectorizers = {'frequency': lambda: nlp516.vectorizer.Unigram(100)} #,\n",
    "               #'presence': lambda: nlp516.vectorizer.UnigramPresence(500)}\n",
    "\n",
    "results = eval_models(classifiers=classifiers, vectorizers=vectorizers,\n",
    "                      task=task, dataset=dataset)\n",
    "print(language, task)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hs = nlp516.model.MlModel(classifier=sklearn.tree.DecisionTreeClassifier(), \n",
    "                                vectorizer=nlp516.vectorizer.Unigram(100))\n",
    "model_ag = nlp516.model.MlModel(classifier=sklearn.linear_model.LogisticRegression(), \n",
    "                                vectorizer=nlp516.vectorizer.Unigram(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hs.fit(dataset.train.text, dataset.train.HS)\n",
    "model_ag.fit(dataset.train.text, dataset.train.AG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(dataset.valid.AG, model_hs.predict(dataset.valid.text) * model_ag.predict(dataset.valid.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nlp516.model.MlModel(classifier=sklearn.tree.DecisionTreeClassifier(), \n",
    "                             vectorizer=nlp516.vectorizer.Unigram(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset.train.text, dataset.train.HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(dataset.valid.text, dataset.valid.HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "[model.vectorizer.id2word(idx[0]) for idx in \n",
    " sorted(enumerate(model.classifier.feature_importances_), key=operator.itemgetter(1))[-100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
